import numpy as np
from scipy import io
# Import datasets, classifiers and performance metrics
from sklearn import datasets, svm, metrics
from sklearn.cross_validation import train_test_split
import matplotlib.pyplot as plt
import math

#assumes binary labels
def entropy(distribution):
	samples = len(distribution)
	if not samples:
		return 0
	positives = float(reduce(lambda x, y: x + y, distribution)) / float(samples)
	negatives = 1 - positives
	if not positives or not negatives:
		return 0
	return -1 * (negatives * math.log(negatives) + positives * math.log(positives))

def impurity_1(left_label_hist, right_label_hist):
	left, right = float(len(left_label_hist)), float(len(right_label_hist))
	return left / (left + right) * entropy(left_label_hist) + right / (left + right) * entropy(right_label_hist)

def best_threshold_func_1(feature, classification, impurity_func):
	pass

def segmentor_1(data, label, impurity_func, best_threshold_func):
	splits = {}
	for feature_column, label in data, label:
		print 'feature_column:', feature_column, 'label', label


class DecisionTree:
	head = None
	def __init__(self, depth, impurity_func = impurity_1, segmentor_func = None):
		self.depth = depth
		self.impurity_func = impurity_func
		self.segmentor_func = segmentor_func
		self.head = Node()
	def split(self, node, training_data, training_labels):
		pass

	def train(self, training_data, training_labels):
		curr_depth = 0
		self.head = split(self.head, training_data, training_labels, depth)
	def predict(self, test_data):
		labels = []
		for item in test_data:
			labels.append(traverse(item))


class Node:
	left = None
	right = None
	split_rule = None
	depth = 0
	def __init__(self):
		pass
class LNode(Node):
	label = None
	def __init__(self, label):
		left = None
		right = None
		self.label = label





